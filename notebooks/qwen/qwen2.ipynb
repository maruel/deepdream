{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969f6d5-b782-4352-b626-2cb913898119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE in (\"cuda\", \"mps\") else torch.float32\n",
    "print(f\"Using {DEVICE}\")\n",
    "\n",
    "def get_generator(seed):\n",
    "    if DEVICE in (\"cuda\", \"mps\"):\n",
    "        return torch.Generator(DEVICE).manual_seed(seed)\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)\n",
    "    return generator\n",
    "\n",
    "#model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "pipe = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=DTYPE)\n",
    "    #torch_dtype=\"auto\",\n",
    "    #device_map=\"auto\"\n",
    "if DEVICE in (\"cuda\", \"mps\"):\n",
    "    pipe = pipe.to(DEVICE)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def chat(messages):\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = pipe.generate(**model_inputs,max_new_tokens=512)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b15e03-b8e7-4942-b201-a971a24bfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "resp = chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc711e2-e893-4e4d-9794-86e27608aec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
